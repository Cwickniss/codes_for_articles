{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12ec631-0d57-4a6d-a0ec-7940abac6968",
   "metadata": {},
   "source": [
    "## 12. Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9021294f-5f48-4f53-8b36-773852531a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.396090</td>\n",
       "      <td>2.092611</td>\n",
       "      <td>2.073392</td>\n",
       "      <td>1.988262</td>\n",
       "      <td>1.953473</td>\n",
       "      <td>2.450997</td>\n",
       "      <td>1.631040</td>\n",
       "      <td>1.746182</td>\n",
       "      <td>1.898050</td>\n",
       "      <td>2.380148</td>\n",
       "      <td>...</td>\n",
       "      <td>1.703454</td>\n",
       "      <td>2.502966</td>\n",
       "      <td>2.119108</td>\n",
       "      <td>2.106098</td>\n",
       "      <td>2.165173</td>\n",
       "      <td>2.340826</td>\n",
       "      <td>2.170109</td>\n",
       "      <td>1.749139</td>\n",
       "      <td>1.678661</td>\n",
       "      <td>1.829647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.775596</td>\n",
       "      <td>1.829438</td>\n",
       "      <td>2.054768</td>\n",
       "      <td>1.577190</td>\n",
       "      <td>1.594549</td>\n",
       "      <td>1.373357</td>\n",
       "      <td>1.946647</td>\n",
       "      <td>1.841420</td>\n",
       "      <td>1.595761</td>\n",
       "      <td>2.538094</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974274</td>\n",
       "      <td>1.621608</td>\n",
       "      <td>2.003085</td>\n",
       "      <td>2.076871</td>\n",
       "      <td>1.788868</td>\n",
       "      <td>2.062829</td>\n",
       "      <td>2.084499</td>\n",
       "      <td>2.267568</td>\n",
       "      <td>1.536939</td>\n",
       "      <td>2.132725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.835679</td>\n",
       "      <td>1.612100</td>\n",
       "      <td>2.174908</td>\n",
       "      <td>2.084460</td>\n",
       "      <td>2.472896</td>\n",
       "      <td>2.029110</td>\n",
       "      <td>2.410107</td>\n",
       "      <td>2.282164</td>\n",
       "      <td>2.208201</td>\n",
       "      <td>2.106240</td>\n",
       "      <td>...</td>\n",
       "      <td>2.035652</td>\n",
       "      <td>2.065291</td>\n",
       "      <td>2.197711</td>\n",
       "      <td>2.288806</td>\n",
       "      <td>2.480274</td>\n",
       "      <td>1.946207</td>\n",
       "      <td>1.947120</td>\n",
       "      <td>1.754344</td>\n",
       "      <td>2.265033</td>\n",
       "      <td>2.119050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.420241</td>\n",
       "      <td>2.158485</td>\n",
       "      <td>1.958602</td>\n",
       "      <td>1.903787</td>\n",
       "      <td>2.230522</td>\n",
       "      <td>1.984789</td>\n",
       "      <td>1.964441</td>\n",
       "      <td>2.360795</td>\n",
       "      <td>1.820773</td>\n",
       "      <td>2.116560</td>\n",
       "      <td>...</td>\n",
       "      <td>2.040977</td>\n",
       "      <td>1.511381</td>\n",
       "      <td>1.834332</td>\n",
       "      <td>2.070046</td>\n",
       "      <td>1.911699</td>\n",
       "      <td>1.816916</td>\n",
       "      <td>2.213950</td>\n",
       "      <td>2.099758</td>\n",
       "      <td>2.259999</td>\n",
       "      <td>2.039066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.646926</td>\n",
       "      <td>1.778067</td>\n",
       "      <td>1.760959</td>\n",
       "      <td>1.894087</td>\n",
       "      <td>1.888225</td>\n",
       "      <td>2.228021</td>\n",
       "      <td>2.489542</td>\n",
       "      <td>2.326377</td>\n",
       "      <td>1.969615</td>\n",
       "      <td>2.001316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.063858</td>\n",
       "      <td>2.341009</td>\n",
       "      <td>1.844115</td>\n",
       "      <td>2.076399</td>\n",
       "      <td>1.742857</td>\n",
       "      <td>1.969530</td>\n",
       "      <td>1.821128</td>\n",
       "      <td>1.946249</td>\n",
       "      <td>1.678283</td>\n",
       "      <td>1.797722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  2.396090  2.092611  2.073392  1.988262  1.953473  2.450997  1.631040   \n",
       "1  1.775596  1.829438  2.054768  1.577190  1.594549  1.373357  1.946647   \n",
       "2  1.835679  1.612100  2.174908  2.084460  2.472896  2.029110  2.410107   \n",
       "3  2.420241  2.158485  1.958602  1.903787  2.230522  1.984789  1.964441   \n",
       "4  1.646926  1.778067  1.760959  1.894087  1.888225  2.228021  2.489542   \n",
       "\n",
       "         7         8         9   ...        15        16        17        18  \\\n",
       "0  1.746182  1.898050  2.380148  ...  1.703454  2.502966  2.119108  2.106098   \n",
       "1  1.841420  1.595761  2.538094  ...  1.974274  1.621608  2.003085  2.076871   \n",
       "2  2.282164  2.208201  2.106240  ...  2.035652  2.065291  2.197711  2.288806   \n",
       "3  2.360795  1.820773  2.116560  ...  2.040977  1.511381  1.834332  2.070046   \n",
       "4  2.326377  1.969615  2.001316  ...  2.063858  2.341009  1.844115  2.076399   \n",
       "\n",
       "         19        20        21        22        23        24  \n",
       "0  2.165173  2.340826  2.170109  1.749139  1.678661  1.829647  \n",
       "1  1.788868  2.062829  2.084499  2.267568  1.536939  2.132725  \n",
       "2  2.480274  1.946207  1.947120  1.754344  2.265033  2.119050  \n",
       "3  1.911699  1.816916  2.213950  2.099758  2.259999  2.039066  \n",
       "4  1.742857  1.969530  1.821128  1.946249  1.678283  1.797722  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.utils.data import generate_data\n",
    "contamination = 0.05 # percentage of outliers\n",
    "n_train = 500       # number of training points\n",
    "n_test = 500        # number of testing points\n",
    "n_features = 25      # number of features\n",
    "X_train, X_test, y_train, y_test = generate_data(\n",
    "    n_train=n_train, \n",
    "    n_test=n_test, \n",
    "    n_features= n_features, \n",
    "    contamination=contamination, \n",
    "    random_state=123)\n",
    "\n",
    "X_train_pd = pd.DataFrame(X_train)\n",
    "X_train_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05bcc7a0-bafc-425e-af0a-e48a584cbf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(5, 3), dpi=80)\n",
    "plt.scatter(X_train_pd[0], X_train_pd[1], c=y_train, alpha=0.8)\n",
    "plt.title('Scatter plot')\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453e735-4c51-4ab3-93e0-a1b83fd15a42",
   "metadata": {},
   "source": [
    "### Step 1: Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea5b38f5-3d40-43b2-a465-1810e5a0b363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 52        \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 25)                75        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 25)                650       \n",
      "=================================================================\n",
      "Total params: 2,733\n",
      "Trainable params: 2,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 3.2389 - val_loss: 2.8298\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 2.7143 - val_loss: 2.5522\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 2.4537 - val_loss: 2.3758\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 2.3011 - val_loss: 2.2636\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 2.2068 - val_loss: 2.1884\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 2.1351 - val_loss: 2.1289\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 2.0801 - val_loss: 2.0733\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 2.0174 - val_loss: 2.0249\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.9745 - val_loss: 1.9779\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.9284 - val_loss: 1.9313\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 141us/sample - loss: 1.8797 - val_loss: 1.8862\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.8362 - val_loss: 1.8372\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.7942 - val_loss: 1.7925\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.7481 - val_loss: 1.7484\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.7037 - val_loss: 1.7011\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.6665 - val_loss: 1.6585\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.6173 - val_loss: 1.6137\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.5804 - val_loss: 1.5716\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.5363 - val_loss: 1.5318\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.5183 - val_loss: 1.4993\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.4912 - val_loss: 1.4706\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.4574 - val_loss: 1.4451\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.4443 - val_loss: 1.4234\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.4076 - val_loss: 1.4041\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.3931 - val_loss: 1.3862\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.3848 - val_loss: 1.3690\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.3594 - val_loss: 1.3540\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.3483 - val_loss: 1.3405\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.3370 - val_loss: 1.3277\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.3189 - val_loss: 1.3162\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.3136 - val_loss: 1.3043\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.3074 - val_loss: 1.2943\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.2903 - val_loss: 1.2842\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.2807 - val_loss: 1.2756\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.2728 - val_loss: 1.2668\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.2631 - val_loss: 1.2594\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.2571 - val_loss: 1.2519\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.2534 - val_loss: 1.2442\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.2505 - val_loss: 1.2381\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.2385 - val_loss: 1.2315\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.2397 - val_loss: 1.2257\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 164us/sample - loss: 1.2266 - val_loss: 1.2206\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.2196 - val_loss: 1.2147\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.2172 - val_loss: 1.2100\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2130 - val_loss: 1.2049\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.2112 - val_loss: 1.2001\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.1979 - val_loss: 1.1957\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.2007 - val_loss: 1.1917\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.1989 - val_loss: 1.1876\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.1883 - val_loss: 1.1836\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1924 - val_loss: 1.1799\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1814 - val_loss: 1.1761\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1850 - val_loss: 1.1725\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1900 - val_loss: 1.1691\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1750 - val_loss: 1.1660\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1768 - val_loss: 1.1626\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1678 - val_loss: 1.1597\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1595 - val_loss: 1.1571\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1656 - val_loss: 1.1546\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1669 - val_loss: 1.1518\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1583 - val_loss: 1.1489\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1479 - val_loss: 1.1463\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1594 - val_loss: 1.1439\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1498 - val_loss: 1.1413\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1509 - val_loss: 1.1388\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1451 - val_loss: 1.1366\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1500 - val_loss: 1.1344\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1417 - val_loss: 1.1325\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1417 - val_loss: 1.1304\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1436 - val_loss: 1.1285\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1421 - val_loss: 1.1266\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1501 - val_loss: 1.1249\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1380 - val_loss: 1.1232\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1258 - val_loss: 1.1216\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1345 - val_loss: 1.1200\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1360 - val_loss: 1.1183\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1348 - val_loss: 1.1167\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1269 - val_loss: 1.1153\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1264 - val_loss: 1.1136\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1267 - val_loss: 1.1120\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1199 - val_loss: 1.1100\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1252 - val_loss: 1.1086\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1170 - val_loss: 1.1073\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1183 - val_loss: 1.1062\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1186 - val_loss: 1.1051\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1193 - val_loss: 1.1039\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1211 - val_loss: 1.1025\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1194 - val_loss: 1.1012\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1225 - val_loss: 1.0999\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1135 - val_loss: 1.0986\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1101 - val_loss: 1.0974\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1127 - val_loss: 1.0963\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1093 - val_loss: 1.0948\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1101 - val_loss: 1.0936\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.1103 - val_loss: 1.0926\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1116 - val_loss: 1.0915\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1085 - val_loss: 1.0905\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1125 - val_loss: 1.0895\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1057 - val_loss: 1.0885\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1001 - val_loss: 1.0875\n",
      "The threshold for the defined contamination rate: 4.122670982828526\n",
      "The training data: {0: 475, 1: 25}\n",
      "The training data: {0: 475, 1: 25}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "atcdr = AutoEncoder(contamination=0.05, hidden_neurons =[25, 2, 2, 25])\n",
    "atcdr.fit(X_train)\n",
    "\n",
    "# Training data\n",
    "y_train_scores = atcdr.decision_function(X_train)\n",
    "y_train_pred = atcdr.predict(X_train)\n",
    "\n",
    "# Test data\n",
    "y_test_scores = atcdr.decision_function(X_test)\n",
    "y_test_pred = atcdr.predict(X_test) # outlier labels (0 or 1)\n",
    "\n",
    "# Threshold for the defined comtanimation rate\n",
    "print(\"The threshold for the defined contamination rate:\" , atcdr.threshold_)\n",
    "\n",
    "def count_stat(vector):\n",
    "    # Because it is '0' and '1', we can run a count statistic. \n",
    "    unique, counts = np.unique(vector, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "print(\"The training data:\", count_stat(y_train_pred))\n",
    "print(\"The training data:\", count_stat(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a150d79-9337-4890-ae82-bb312d166a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'contamination': 0.05,\n",
       " 'dropout_rate': 0.2,\n",
       " 'epochs': 100,\n",
       " 'hidden_activation': 'relu',\n",
       " 'hidden_neurons': [25, 2, 2, 25],\n",
       " 'l2_regularizer': 0.1,\n",
       " 'loss': <function tensorflow.python.keras.losses.mean_squared_error(y_true, y_pred)>,\n",
       " 'optimizer': 'adam',\n",
       " 'output_activation': 'sigmoid',\n",
       " 'preprocessing': True,\n",
       " 'random_state': None,\n",
       " 'validation_size': 0.1,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atcdr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1943f6f-327d-497b-9a4a-5ef69864ddc1",
   "metadata": {},
   "source": [
    "### Step 2: Determine the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9015e76e-d382-4bfb-867f-96eccef1d5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train_scores, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Outlier score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33284af5-5a22-465a-ad83-78bb81e3cd9e",
   "metadata": {},
   "source": [
    "### Step 3: Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82b6f4a1-7215-4874-a32a-450a70ff1af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>20.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5    6  ...  \\\n",
       "0   Normal    475     95.0  2.01  2.01  2.02  2.00  2.00  2.00  2.0  ...   \n",
       "1  Outlier     25      5.0 -0.02  0.21 -0.05  0.13  0.01 -0.32  0.2  ...   \n",
       "\n",
       "     16    17    18    19    20    21   22    23    24  Anomaly_Score  \n",
       "0  1.99  2.02  2.01  2.00  1.99  2.00  2.0  2.01  1.99           2.13  \n",
       "1 -0.07 -0.30  0.11  0.18  0.14 -0.28  0.2  0.09 -0.10          20.31  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = atcdr.threshold_ # Or other value from the above histogram\n",
    "\n",
    "def descriptive_stat_threshold(df,pred_score, threshold):\n",
    "    # Let's see how many '0's and '1's.\n",
    "    df = pd.DataFrame(df)\n",
    "    df['Anomaly_Score'] = pred_score\n",
    "    df['Group'] = np.where(df['Anomaly_Score']< threshold, 'Normal', 'Outlier')\n",
    "\n",
    "    # Now let's show the summary statistics:\n",
    "    cnt = df.groupby('Group')['Anomaly_Score'].count().reset_index().rename(columns={'Anomaly_Score':'Count'})\n",
    "    cnt['Count %'] = (cnt['Count'] / cnt['Count'].sum()) * 100 # The count and count %\n",
    "    stat = df.groupby('Group').mean().round(2).reset_index() # The avg.\n",
    "    stat = cnt.merge(stat, left_on='Group',right_on='Group') # Put the count and the avg. together\n",
    "    return (stat)\n",
    "\n",
    "descriptive_stat_threshold(X_train,y_train_scores, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9a42470-3753-4067-a35d-9832310d5379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    475     95.0  2.00  1.98  2.02  2.01  1.99  2.01  1.99  ...   \n",
       "1  Outlier     25      5.0 -0.22 -0.45  0.04 -0.26 -0.34 -0.14  0.12  ...   \n",
       "\n",
       "     16   17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  2.00  2.0  2.01  2.00  2.00  1.99  2.01  1.98  1.99           2.12  \n",
       "1 -0.07 -0.1  0.15  0.02 -0.36  0.12  0.50 -0.08  0.36          20.96  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_test,y_test_scores, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bf7362d-5fd3-4151-a61e-b148a7607c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix(actual,score, threshold):\n",
    "    Actual_pred = pd.DataFrame({'Actual': actual, 'Pred': score})\n",
    "    Actual_pred['Pred'] = np.where(Actual_pred['Pred']<=threshold,0,1)\n",
    "    cm = pd.crosstab(Actual_pred['Actual'],Actual_pred['Pred'])\n",
    "    return (cm)\n",
    "confusion_matrix(y_train,y_train_scores,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de5405d9-690b-43ad-a4a1-6c32156ac9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_test_scores,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91fcbc-aa09-4c08-b0c7-d77310ccc904",
   "metadata": {},
   "source": [
    "### Step 4: Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73d2bb66-b6ef-44fc-9c9f-0dbfb4b484a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 52        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 25)                75        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 25)                650       \n",
      "=================================================================\n",
      "Total params: 2,733\n",
      "Trainable params: 2,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 3.8289 - val_loss: 1.2058\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 141us/sample - loss: 3.1705 - val_loss: 1.1445\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 2.8369 - val_loss: 1.0961\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 2.5675 - val_loss: 1.0502\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 2.4155 - val_loss: 1.0042\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 2.2686 - val_loss: 0.9543\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 2.1571 - val_loss: 0.9001\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 2.0565 - val_loss: 0.8426\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 140us/sample - loss: 1.9665 - val_loss: 0.7852\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.8732 - val_loss: 0.7321\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.8113 - val_loss: 0.6865\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.7423 - val_loss: 0.6476\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.7015 - val_loss: 0.6137\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.6433 - val_loss: 0.5838\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.6071 - val_loss: 0.5579\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.5680 - val_loss: 0.5364\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.5377 - val_loss: 0.5169\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.5100 - val_loss: 0.5002\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.4957 - val_loss: 0.4854\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.4705 - val_loss: 0.4717\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.4548 - val_loss: 0.4588\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.4322 - val_loss: 0.4473\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.4193 - val_loss: 0.4371\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.4039 - val_loss: 0.4274\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.3956 - val_loss: 0.4185\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.3848 - val_loss: 0.4101\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.3727 - val_loss: 0.4022\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.3610 - val_loss: 0.3945\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.3580 - val_loss: 0.3885\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.3426 - val_loss: 0.3813\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.3358 - val_loss: 0.3745\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.3266 - val_loss: 0.3682\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.3198 - val_loss: 0.3628\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.3136 - val_loss: 0.3577\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.3100 - val_loss: 0.3526\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.3031 - val_loss: 0.3503\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2954 - val_loss: 0.3450\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.2906 - val_loss: 0.3402\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.2892 - val_loss: 0.3363\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.2807 - val_loss: 0.3317\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2694 - val_loss: 0.3278\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.2705 - val_loss: 0.3248\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2700 - val_loss: 0.3256\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.2623 - val_loss: 0.3184\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.2608 - val_loss: 0.3148\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.2568 - val_loss: 0.3118\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.2523 - val_loss: 0.3090\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2483 - val_loss: 0.3069\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.2447 - val_loss: 0.3042\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.2446 - val_loss: 0.3014\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2406 - val_loss: 0.2993\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2372 - val_loss: 0.2969\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.2302 - val_loss: 0.2946\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2329 - val_loss: 0.2927\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2278 - val_loss: 0.2907\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2271 - val_loss: 0.2886\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2217 - val_loss: 0.2866\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.2244 - val_loss: 0.2850\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.2190 - val_loss: 0.2835\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.2189 - val_loss: 0.2821\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.2134 - val_loss: 0.2796\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.2117 - val_loss: 0.2779\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.2129 - val_loss: 0.2775\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.2060 - val_loss: 0.2758\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.2049 - val_loss: 0.2743\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.2070 - val_loss: 0.2722\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.2060 - val_loss: 0.2707\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.2017 - val_loss: 0.2694\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.2026 - val_loss: 0.2691\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.1979 - val_loss: 0.2672\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1958 - val_loss: 0.2657\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1954 - val_loss: 0.2644\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.1968 - val_loss: 0.2637\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1863 - val_loss: 0.2620\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1909 - val_loss: 0.2623\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1910 - val_loss: 0.2612\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1882 - val_loss: 0.2595\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1854 - val_loss: 0.2586\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.1843 - val_loss: 0.2573\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1860 - val_loss: 0.2566\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.1834 - val_loss: 0.2550\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.1806 - val_loss: 0.2543\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1840 - val_loss: 0.2531\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1787 - val_loss: 0.2533\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1794 - val_loss: 0.2517\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1758 - val_loss: 0.2510\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1717 - val_loss: 0.2501\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1790 - val_loss: 0.2496\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.1770 - val_loss: 0.2486\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1770 - val_loss: 0.2476\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1739 - val_loss: 0.2471\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1725 - val_loss: 0.2468\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.1704 - val_loss: 0.2461\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1709 - val_loss: 0.2458\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.1725 - val_loss: 0.2454\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1644 - val_loss: 0.2446\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.1673 - val_loss: 0.2440\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1700 - val_loss: 0.2436\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1623 - val_loss: 0.2428\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.1713 - val_loss: 0.2442\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 25)                275       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 25)                650       \n",
      "=================================================================\n",
      "Total params: 3,187\n",
      "Trainable params: 3,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 3.5905 - val_loss: 1.8027\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 3.0242 - val_loss: 1.6340\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 2.6738 - val_loss: 1.5253\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 162us/sample - loss: 2.4431 - val_loss: 1.4418\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 163us/sample - loss: 2.3169 - val_loss: 1.3631\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 2.1720 - val_loss: 1.2848\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 2.0412 - val_loss: 1.2040\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.9194 - val_loss: 1.1251\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.8139 - val_loss: 1.0576\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.7313 - val_loss: 1.0059\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.6589 - val_loss: 0.9668\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 175us/sample - loss: 1.6178 - val_loss: 0.9372\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 164us/sample - loss: 1.5668 - val_loss: 0.9119\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.5397 - val_loss: 0.8902\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.5077 - val_loss: 0.8698\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.4808 - val_loss: 0.8522\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.4520 - val_loss: 0.8361\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.4340 - val_loss: 0.8202\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.4166 - val_loss: 0.8078\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.3951 - val_loss: 0.7953\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.3792 - val_loss: 0.7830\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.3617 - val_loss: 0.7717\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.3506 - val_loss: 0.7613\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.3379 - val_loss: 0.7516\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.3224 - val_loss: 0.7435\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.3171 - val_loss: 0.7346\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.3098 - val_loss: 0.7277\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2941 - val_loss: 0.7200\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2915 - val_loss: 0.7129\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.2762 - val_loss: 0.7060\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.2720 - val_loss: 0.6995\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.2639 - val_loss: 0.6941\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.2535 - val_loss: 0.6913\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.2468 - val_loss: 0.6851\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.2421 - val_loss: 0.6790\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.2364 - val_loss: 0.6737\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2316 - val_loss: 0.6694\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.2287 - val_loss: 0.6653\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.2189 - val_loss: 0.6612\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2173 - val_loss: 0.6567\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.2132 - val_loss: 0.6530\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.2043 - val_loss: 0.6498\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.2028 - val_loss: 0.6466\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.2016 - val_loss: 0.6436\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1989 - val_loss: 0.6403\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1930 - val_loss: 0.6377\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1877 - val_loss: 0.6346\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.1884 - val_loss: 0.6327\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1858 - val_loss: 0.6329\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1809 - val_loss: 0.6303\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1783 - val_loss: 0.6287\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1776 - val_loss: 0.6257\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1731 - val_loss: 0.6227\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.1720 - val_loss: 0.6202\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1703 - val_loss: 0.6179\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1685 - val_loss: 0.6158\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1667 - val_loss: 0.6141\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1627 - val_loss: 0.6125\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1625 - val_loss: 0.6106\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1613 - val_loss: 0.6090\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1588 - val_loss: 0.6076\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1566 - val_loss: 0.6062\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1540 - val_loss: 0.6048\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1523 - val_loss: 0.6034\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1522 - val_loss: 0.6021\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1482 - val_loss: 0.6007\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.1503 - val_loss: 0.5996\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.1489 - val_loss: 0.5985\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 162us/sample - loss: 1.1475 - val_loss: 0.5970\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1448 - val_loss: 0.5961\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1433 - val_loss: 0.5948\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1420 - val_loss: 0.5952\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.1393 - val_loss: 0.5937\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1374 - val_loss: 0.5923\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1430 - val_loss: 0.5939\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1363 - val_loss: 0.5908\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.1355 - val_loss: 0.5889\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1345 - val_loss: 0.5878\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1306 - val_loss: 0.5867\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1327 - val_loss: 0.5860\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1338 - val_loss: 0.5852\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.1327 - val_loss: 0.5847\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1309 - val_loss: 0.5837\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1280 - val_loss: 0.5825\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1289 - val_loss: 0.5819\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1285 - val_loss: 0.5813\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1235 - val_loss: 0.5805\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1252 - val_loss: 0.5812\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1252 - val_loss: 0.5804\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1228 - val_loss: 0.5792\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.1233 - val_loss: 0.5781\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1226 - val_loss: 0.5777\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1217 - val_loss: 0.5766\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1200 - val_loss: 0.5760\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1190 - val_loss: 0.5787\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.1202 - val_loss: 0.5748\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1175 - val_loss: 0.5739\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1194 - val_loss: 0.5736\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1186 - val_loss: 0.5729\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1196 - val_loss: 0.5730\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 15)                390       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 25)                400       \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 25)                650       \n",
      "=================================================================\n",
      "Total params: 3,767\n",
      "Trainable params: 3,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 2s 4ms/sample - loss: 3.7761 - val_loss: 2.0959\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 181us/sample - loss: 3.0286 - val_loss: 1.8695\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 2.6846 - val_loss: 1.7241\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 2.4956 - val_loss: 1.6022\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 2.2902 - val_loss: 1.4910\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 179us/sample - loss: 2.1126 - val_loss: 1.3813\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.9856 - val_loss: 1.2843\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.8698 - val_loss: 1.2075\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 175us/sample - loss: 1.7768 - val_loss: 1.1492\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.6991 - val_loss: 1.1036\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.6444 - val_loss: 1.0654\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 1.6017 - val_loss: 1.0333\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 182us/sample - loss: 1.5631 - val_loss: 1.0043\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 1.5308 - val_loss: 0.9769\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 1.5006 - val_loss: 0.9539\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 1.4730 - val_loss: 0.9305\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 1.4414 - val_loss: 0.9107\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 1.4299 - val_loss: 0.8953\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 1.4053 - val_loss: 0.8785\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 1.3911 - val_loss: 0.8660\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 1.3793 - val_loss: 0.8516\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 1.3616 - val_loss: 0.8378\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 1.3449 - val_loss: 0.8252\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 1.3317 - val_loss: 0.8144\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 1.3259 - val_loss: 0.8045\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 180us/sample - loss: 1.3138 - val_loss: 0.7949\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 1.3067 - val_loss: 0.7907\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 1.2959 - val_loss: 0.7825\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 1.2871 - val_loss: 0.7743\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 1.2816 - val_loss: 0.7650\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 176us/sample - loss: 1.2717 - val_loss: 0.7565\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 175us/sample - loss: 1.2653 - val_loss: 0.7495\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.2580 - val_loss: 0.7432\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 1.2516 - val_loss: 0.7375\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.2459 - val_loss: 0.7313\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 1.2369 - val_loss: 0.7254\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.2309 - val_loss: 0.7202\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 1.2282 - val_loss: 0.7151\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.2236 - val_loss: 0.7110\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.2200 - val_loss: 0.7067\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.2158 - val_loss: 0.7025\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 1.2124 - val_loss: 0.7012\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 1.2059 - val_loss: 0.6957\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 1.2065 - val_loss: 0.6909\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 182us/sample - loss: 1.1978 - val_loss: 0.6884\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 1.1971 - val_loss: 0.6876\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 1.1953 - val_loss: 0.6817\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 1.1906 - val_loss: 0.6777\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 336us/sample - loss: 1.1865 - val_loss: 0.6745\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 295us/sample - loss: 1.1828 - val_loss: 0.6718\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 308us/sample - loss: 1.1814 - val_loss: 0.6693\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 1.1806 - val_loss: 0.6663\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 1.1772 - val_loss: 0.6636\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 1.1729 - val_loss: 0.6611\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 1.1713 - val_loss: 0.6586\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 1.1763 - val_loss: 0.6567\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 1.1681 - val_loss: 0.6544\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 182us/sample - loss: 1.1663 - val_loss: 0.6537\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 1.1635 - val_loss: 0.6513\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 1.1642 - val_loss: 0.6506\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 180us/sample - loss: 1.1605 - val_loss: 0.6469\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 1.1580 - val_loss: 0.6442\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.1575 - val_loss: 0.6419\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.1560 - val_loss: 0.6403\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 1.1495 - val_loss: 0.6387\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 1.1490 - val_loss: 0.6373\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 1.1517 - val_loss: 0.6356\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 181us/sample - loss: 1.1492 - val_loss: 0.6345\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.1458 - val_loss: 0.6330\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 179us/sample - loss: 1.1466 - val_loss: 0.6316\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.1429 - val_loss: 0.6303\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.1428 - val_loss: 0.6288\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 175us/sample - loss: 1.1404 - val_loss: 0.6275\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 175us/sample - loss: 1.1405 - val_loss: 0.6267\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 1.1399 - val_loss: 0.6253\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 1.1376 - val_loss: 0.6239\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 254us/sample - loss: 1.1351 - val_loss: 0.6229\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 1.1379 - val_loss: 0.6220\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 1.1346 - val_loss: 0.6209\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 1.1345 - val_loss: 0.6197\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 209us/sample - loss: 1.1356 - val_loss: 0.6188\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 209us/sample - loss: 1.1327 - val_loss: 0.6179\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 179us/sample - loss: 1.1311 - val_loss: 0.6169\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 181us/sample - loss: 1.1272 - val_loss: 0.6159\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 179us/sample - loss: 1.1286 - val_loss: 0.6149\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.1277 - val_loss: 0.6150\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 1.1281 - val_loss: 0.6138\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 1.1258 - val_loss: 0.6128\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 179us/sample - loss: 1.1252 - val_loss: 0.6115\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 175us/sample - loss: 1.1260 - val_loss: 0.6104\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 176us/sample - loss: 1.1230 - val_loss: 0.6095\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.1222 - val_loss: 0.6090\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 179us/sample - loss: 1.1242 - val_loss: 0.6085\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 1.1211 - val_loss: 0.6075\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 176us/sample - loss: 1.1216 - val_loss: 0.6068\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 176us/sample - loss: 1.1187 - val_loss: 0.6061\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 1.1186 - val_loss: 0.6061\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 1.1186 - val_loss: 0.6055\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 176us/sample - loss: 1.1166 - val_loss: 0.6043\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 176us/sample - loss: 1.1186 - val_loss: 0.6033\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "atcdr1 = AutoEncoder(contamination=0.05, hidden_neurons =[25, 2, 2, 25])\n",
    "atcdr2 = AutoEncoder(contamination=0.05, hidden_neurons =[25, 10, 2, 10, 25])\n",
    "atcdr3 = AutoEncoder(contamination=0.05, hidden_neurons =[25, 15, 10, 2, 10, 15, 25] )\n",
    "\n",
    "# Standardize data\n",
    "X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
    "\n",
    "# Just prepare data frames so we can store the model results. There are three models.\n",
    "train_scores = np.zeros([X_train.shape[0], 3])\n",
    "test_scores = np.zeros([X_test.shape[0], 3])\n",
    "atcdr1.fit(X_train_norm)\n",
    "atcdr2.fit(X_train_norm)\n",
    "atcdr3.fit(X_train_norm)\n",
    "    \n",
    "# Store the results in each column:\n",
    "train_scores[:, 0] = atcdr1.decision_function(X_train_norm) \n",
    "train_scores[:, 1] = atcdr2.decision_function(X_train_norm) \n",
    "train_scores[:, 2] = atcdr3.decision_function(X_train_norm)\n",
    "test_scores[:, 0] = atcdr1.decision_function(X_test_norm) \n",
    "test_scores[:, 1] = atcdr2.decision_function(X_test_norm) \n",
    "test_scores[:, 2] = atcdr3.decision_function(X_test_norm)\n",
    "\n",
    "# Decision scores have to be normalized before combination\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores,test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ec9923e-c4df-432e-96dd-e92e5b66497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "# Combination by average\n",
    "y_train_by_average = average(train_scores_norm)\n",
    "y_test_by_average = average(test_scores_norm)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train_by_average, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37c2b2e2-ebe3-490c-8008-c9984613a60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>473</td>\n",
       "      <td>94.6</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>27</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    473     94.6  2.01  2.01  2.02  2.00  2.00  2.00  2.00  ...   \n",
       "1  Outlier     27      5.4  0.13  0.39  0.09  0.25  0.14 -0.16  0.35  ...   \n",
       "\n",
       "     16    17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  1.99  2.02  2.01  2.00  1.99  2.00  2.00  2.01  1.99          -0.23  \n",
       "1  0.08 -0.12  0.27  0.32  0.27 -0.09  0.35  0.24  0.04           4.00  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_train,y_train_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6a23053-4d1a-4d3e-992a-416439aa5ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>474</td>\n",
       "      <td>94.8</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>26</td>\n",
       "      <td>5.2</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.43</td>\n",
       "      <td>4.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    474     94.8  2.00  1.98  2.02  2.01  1.99  2.01  1.99  ...   \n",
       "1  Outlier     26      5.2 -0.12 -0.35  0.11 -0.16 -0.25 -0.05  0.20  ...   \n",
       "\n",
       "     16    17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  2.00  2.00  2.01  2.00  2.00  1.99  2.01  1.98  1.99          -0.23  \n",
       "1  0.01  0.01  0.21  0.12 -0.27  0.18  0.55  0.01  0.43           4.31  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_test,y_test_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19589ca4-8802-4014-9d14-3c378f31c39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>473</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     473   2\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,y_train_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31f86490-1377-4b2c-8fad-21c622a33e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     474   1\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_test_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196290e-37dd-4328-a264-25b0d1b75436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
